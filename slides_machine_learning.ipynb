{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169ce4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039b4d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning\n",
    "* Enables problem solving:\n",
    "    * the transition of a state with respect to a quality factor\n",
    "    * state x is changed with some function\n",
    "    * f(speech) = emotion\n",
    "    * f(x) = x'\n",
    "    * e.g. f(x) = a x + b\n",
    "    * find a and b so that x' is optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465730d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to split your data\n",
    "In supervised machine learning, you usually need three kinds of data sets:\n",
    "* train data: to teach the model the relation between data and labels\n",
    "* dev data: (short for *development*) to tune meta parameters of your model, e.g. \n",
    "    * *number of neurons*, \n",
    "    * *batch size* or \n",
    "    * *learning rate*.\n",
    "* test data: to evaluate your model ONCE at the end to check on generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e5047",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course all this is to prevent [*overfitting*](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Overfitting) on your train and/or dev data.\n",
    "\n",
    "* If you've used your test data for a while, \n",
    "you might need to find a new set, \n",
    "as chances are high that you overfitted \n",
    "on your test during experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32991ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So what's a good split?\n",
    "\n",
    "Some rules apply:\n",
    "* train and dev can be from the same set, but the test set is ideally from a different database.\n",
    "* if you don't have so much data, a 80/20/20 % split is normal\n",
    "* if you have masses an data, use only so much dev and test that your population seems covered.\n",
    "* If you have really little data: use [x cross validation](http://blog.syntheticspeech.de/2022/11/28/how-to-evaluate-your-model/#X_fold_cross_validation) for train and dev, still the test set should be extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b56745",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nkululeko exercise 1\n",
    "\n",
    "\n",
    "Edit the [demo configuration](https://github.com/felixbur/nkululeko/blob/main/demos/exp_emodb.ini)\n",
    "\n",
    "Set/keep as target *emotion* as FEAT type *os* and as MODEL type *xgb*\n",
    "\n",
    "Use the emodb as test and train set but try [out all split methods](https://github.com/felixbur/nkululeko/blob/main/ini_file.md#data)\n",
    "* specified\n",
    "* speaker split\n",
    "* random\n",
    "* loso\n",
    "* logo \n",
    "* 5_fold_cross_validation\n",
    "\n",
    "Which works best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da536c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nkululeko exercise 2\n",
    "Set the \n",
    "```\n",
    "[EXP]\n",
    "epochs = 200\n",
    "[MODEL] \n",
    "type = mlp\n",
    "layers = {'l1':1024, 'l2':64} \n",
    "save = True\n",
    "[PLOT]\n",
    "epoch_progression = True\n",
    "best_model = True\n",
    "```\n",
    "run the experiment.\n",
    "Find the epoch progression plot and see at which epoch overfitting starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c4f94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "These slides about evaluation of machine learning models, obviously the answer to the question if a model is any good depends a lot on how you test that.\n",
    "\n",
    "## Criteria\n",
    "Depending whether you got a classification or regression problem you can choose from a multitude of measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3ab49",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classification\n",
    "Most of these measures are derived from the confusion matrix:\n",
    "* **Confusion Matrix** : Matrix with results: rows represent the real values and columns the predictions. \n",
    "* In the binary case, the cells are called *True Positive* (TP), *False Negative* (FN: Type 2 error), *False Positive* (FN: Type 1 error) and *True Negative* (TN)\n",
    "* So in the example (next slide), TP=3, FN=4, FP=3 and TN=3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636f722",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/Prec-recall.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e70065",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/conf_mat.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea47d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following measurements can be derived from these:\n",
    "\n",
    "* **Accuracy**: Percentage of correct predictions -> (TP+TN)/(TP+FP+FN+TN).\n",
    "* **un- / weighted Recall/Sensitivity**: percentage of detected cases -> TP / (TP+FN). Can be weighted by class frequency, for multiple classes \n",
    "* **un- / weighted Precision**: percentage of relevant predictions -> TP / (TP+FP)\n",
    "* **Specificity**: Like sensitivity, but for the negative examples -> TN / (TN+FP)\n",
    "* **F1**: Combination of Recall and Precision -> F1 = 2 * (Rec* Prec)/ (Rec + Prec) \n",
    "* **AUC/ROC** Usually there's a tradeoff between Recall and Precision. With the *Receiver Operator Curve*  and it's *Area under curve* this can be visualized by plotting the False positive rate (100-specificity) against the True positive rate (sensitivity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b8dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression\n",
    "* **Pearson's** Pearson's Correlation Coefficient  measures the similarity of two sets of numbers with the same lenght. I's a value between -1 and 1, with 0 meaning no correlation and -1 negative correlation. When plotted in 2-d space, PCC=1 would be the identity line.\n",
    "\n",
    "* **MAE** Mean absolute error: taken two sets of numbers with same length as correct and predicted values, one can compute the mean absolute error by summing up the absolute values of the pairwise differences and scale by the number of samples.\n",
    "\n",
    "* **CCC** Concordance Correlation Coefficient is a measure quite similar to PCC but tries to penalize rater bias (seeing the two distributions as truth and ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc54d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Approaches\n",
    "### Train / test /dev splits\n",
    "Best you have enough data to split it into seperate sets:\n",
    "* **train** for the training\n",
    "* **dev** to tune meta-parameters\n",
    "* **test** as a final test set \n",
    "\n",
    "Be careful to make sure that they are  speaker disjunct, i.e. not have overlapping speakers, else you can't be sure if you learn general speaker characteristics or speaker idiosyncrasies.\n",
    "\n",
    "Also it's a very good idea to have the test set from a completely different data source, so you could have more trust in the generalizability of your model.\n",
    "\n",
    "More [on the subject here](http://blog.syntheticspeech.de/2022/12/01/how-to-split-you-data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73ef57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### X fold cross validation\n",
    "\n",
    "* If you are low on data, you might try x fold cross validation, \n",
    "* it means that you split your data  in *x* (usually 10) sets with same size, \n",
    "* and then do *x* trainings, \n",
    "* using one set as *dev set* and the rest for *train*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc46a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LOSO\n",
    "\n",
    "* *Leave one Speaker out* \n",
    "* is like X fold cross-validation, but each set are all samples of one speaker. \n",
    "* If there are many speakers, you might want *Leave one speaker group out*.\n",
    "Both is supported by [Nkululeko](http://blog.syntheticspeech.de/2021/08/04/machine-learning-experiment-framework/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8728f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different machine learners\n",
    "* gives an overview on popular machine learners in a nutshell.\n",
    "* Lots of site on the internet give great detail on this and you should take a few minuted to check them out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc854e57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In general, all these approaches work by extracting *features* from data and \n",
    "* comparing a *test* sample's features with the features derived from a *training* set \n",
    "* to predict some *class*  or *value*  [in case of regression](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Classification_vs_Regression).\n",
    "\n",
    "So they work with two phases: \n",
    "\n",
    "* During training, the *parameters* of the approach are learned, thereby creating the *model*.\n",
    "* A test time, unknown test samples get *predicted* by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d91fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, most of these approaches can be customized by \n",
    "* *meta-parameters* which also can be learned by some \n",
    "* *meta algorithm*, but not during a normal training. \n",
    "\n",
    "One thing all of these approaches have in common is that they \n",
    "* *model* the world by \"densing\" down  the real values, \n",
    "* i.e. the data, to a simpler form at some time (*feature extraction*), \n",
    "\n",
    "so they all can be seen as some kind of dimensionality reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3126b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* On the one hand you lose information this way, \n",
    "* on the other this is not a problem because you usually are interested in some kind of underlying principle that generated your training data, \n",
    "* and not so much in the training data itself.\n",
    "\n",
    "Still you got a trade-off between \n",
    "* generalizability and\n",
    "* [specificity](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Bias_vs_variance)\n",
    "* aka *bias* vs. *variance*\n",
    "\n",
    "The following list is by far not complete, I simply selected the ones that were most commonly used during my professional life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8e40e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear regression\n",
    "To represent the dependency of a dependend and an independend variable by a straight line. \n",
    "* The price question is how to learn the two parameters of  the line (*a* and *b* of *y=ax+b*) using the training data. \n",
    "* One approach would be gradient descent with a [Perceptron](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Perceptron).\n",
    "<img src=images/linear_regression.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b79de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GMMs\n",
    "\n",
    "A Gaussian is a way to describe \n",
    "* a distribution with two values: mean and variance. \n",
    "* One way to distinguish two kinds of things is two distinguish them by the distributions of their features,\n",
    "* e.g. herrings from trouts by the size of their fins.\n",
    "* Gaussian mixture models model *one* distribution of each feature by a mix of several Gaussians, hence their name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b853a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/gmms.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98c97d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (Naive) Bayes\n",
    "* Bayes statistics is fundamentally different from so-called frequentist statistics, as it takes prior knowledge of the problem into account.\n",
    "* The Bayesian formula tells us how likely an event (the class we want to distinguish) can happen in conjunction with another event (the feature that we observe).\n",
    "* During training the Bayes classifier updates its believe about the world, using absolute or estimated frequencies as prior knowledge.\n",
    "* The approach is called naive because it assumes that each input feature is independent, which is most of the time not true.\n",
    "<img src=images/naive_bayes.png width=20%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9282d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KNN (k nearest neighbor)\n",
    "\n",
    "* K nearest neighbor is an approach to assign *test* data, \n",
    "* given its *k* (given parameter) nearest neighbors (in the *feature* space, by some distance metrics) \n",
    "* either the most common *class* or some property *value* as an average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af57950",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/knn.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c27092",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Support vector machines\n",
    "* Support vector machines are algorithms motivated by vector geometry\n",
    "* They construct *hyperplanes* in N-dimensional (number of *features*) space by maximizing the margin between data points from different classes.\n",
    "* The function that defines the hyperplane is called the kernel function and can be parameterized.\n",
    "* They can be combined with GMMS if the data is approximated by them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417dc713",
   "metadata": {},
   "source": [
    "<img src=images/svm.png width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c3197",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CART (classification and regression trees)\n",
    "* Perhaps the most straightforward way to categorize data: order its parameters in a tree like fashion with the *features* as twigs and the data points as leaves.\n",
    "* The tree is learned from the training set (and can be probabilistic).\n",
    "* The big advantage of this model is that it is easily interpretable to humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849d1d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/cart.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919884",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## XGBoost\n",
    "* A sophisticated algorithm loosely based on CARTS \n",
    "* as it combines Random Forests (ensembles of trees) \n",
    "* with boosting more successful ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31120d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/xgboost.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450157d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MLP (Multi-layer perceptron)\n",
    "\n",
    "As the name suggests, these algorithms are derived from the original [Perceptron](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Perceptron) idea that is inspired by the human brain.\n",
    "<img src=images/ann.png width=40%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb0307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
