{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169ce4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039b4d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning\n",
    "* Enables problem solving:\n",
    "    * the transition of a state with respect to a quality factor\n",
    "    * state x is changed with some function\n",
    "    * f(speech) = emotion\n",
    "    * f(x) = x'\n",
    "    * e.g. f(x) = a x + b\n",
    "    * find a and b so that x' is optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed57455",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26e52d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function\n",
    " is the function that artificial neural nets use to track progress, i.e. the function that evaluates the predicted outcome with the desired one. Finding a good loss function is crucial for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87be2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "Fundamental way to train neural networks by evaluating the error with the loss function and than propagating it backwards towards the input layer, by taking the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f616679",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch size\n",
    "number of samples in one batch in the training which are used together to compute the error (-> loss function) and do the backpropagation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6b464",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Embeddings\n",
    " are learned representations of data, usually the pen-ultimate layer of a pretrained artificial neural net. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b215c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latent space\n",
    " means the property of deep artificial neural nets to represent specific features of the data within the higher layers, for example speaker characteristics or expressed emotion in a net trained for speech synthesis. This is often used to influence the output in a desired way, for example simulating a specific speaking style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6cf51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Freezing\n",
    " layers in an ANN means to not update the weights, as they might contain knowledge that should not be forgotten (from a pretrained net) or to make the training faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5be0ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Drop out\n",
    "is the technique to delete a number of randomly selected neurons in a hidden layer during training to prevent [overfitting](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2133df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Patience \n",
    "* Number of epochs with no improvement after which training will be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc3eeb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overfitting\n",
    "* Means that the machine learner performs well on the training but not on any other data. \n",
    "* This is usually the case when the model has enough complexity to distinguish all training data and is trained for enough periods (one period is one run through the training). \n",
    "* Measures against this are subsumed under the label *regularization*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fbd54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vanishing / exploding gradient \n",
    "Means that the weights of the neurons become too small or too large for the net to be stable. \n",
    "\n",
    "This happens especially with very deep (many layers) networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819ac51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias vs. variance \n",
    " means the trade-off between generalization (high bias, underfitting) and specification (high variance, overfitting). You can either \n",
    " \n",
    "* have simple models, like e.g. linear regression classifiers, that will treat every input with a similar strong bias (wrong decisions), irrespective of the training set, or \n",
    "* very complex models (e.g. a neural net with many layers) that will be more exact but very specific to your training data.\n",
    "\n",
    "[Here](https://mlu-explain.github.io/bias-variance/)'s a nice visualization of bias vs. variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465730d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to split your data\n",
    "In supervised machine learning, you usually need three kinds of data sets:\n",
    "* train data: to teach the model the relation between data and labels\n",
    "* dev data: (short for *development*) to tune meta parameters of your model, e.g. \n",
    "    * *number of neurons*, \n",
    "    * *batch size* or \n",
    "    * *learning rate*.\n",
    "* test data: to evaluate your model ONCE at the end to check on generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e5047",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course all this is to prevent [*overfitting*](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Overfitting) on your train and/or dev data.\n",
    "\n",
    "* If you've used your test data for a while, \n",
    "you might need to find a new set, \n",
    "as chances are high that you overfitted \n",
    "on your test during experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32991ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So what's a good split?\n",
    "\n",
    "Some rules apply:\n",
    "* train and dev can be from the same set, but the test set is ideally from a different database.\n",
    "* if you don't have so much data, a 80/20/20 % split is normal\n",
    "* if you have masses an data, use only so much dev and test that your population seems covered.\n",
    "* If you have really little data: use [x cross validation](http://blog.syntheticspeech.de/2022/11/28/how-to-evaluate-your-model/#X_fold_cross_validation) for train and dev, still the test set should be extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b56745",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nkululeko exercise 1\n",
    "\n",
    "\n",
    "Edit the [demo configuration](https://github.com/felixbur/nkululeko/blob/main/demos/exp_emodb.ini)\n",
    "\n",
    "Set/keep as target *emotion* as FEAT type *os* and as MODEL type *xgb*\n",
    "\n",
    "Use the emodb as test and train set but try [out all split methods](https://github.com/felixbur/nkululeko/blob/main/ini_file.md#data)\n",
    "* specified\n",
    "* speaker split\n",
    "* random\n",
    "* loso\n",
    "* logo \n",
    "* 5_fold_cross_validation\n",
    "\n",
    "Which works best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da536c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nkululeko exercise 2\n",
    "Set the \n",
    "```\n",
    "[EXP]\n",
    "epochs = 200\n",
    "[MODEL] \n",
    "type = mlp\n",
    "layers = {'l1':1024, 'l2':64} \n",
    "save = True\n",
    "[PLOT]\n",
    "epoch_progression = True\n",
    "best_model = True\n",
    "```\n",
    "run the experiment.\n",
    "Find the epoch progression plot and see at which epoch overfitting starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c4f94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "These slides about evaluation of machine learning models, obviously the answer to the question if a model is any good depends a lot on how you test that.\n",
    "\n",
    "## Criteria\n",
    "Depending whether you got a classification or regression problem you can choose from a multitude of measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3ab49",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classification\n",
    "Most of these measures are derived from the confusion matrix:\n",
    "* **Confusion Matrix** : Matrix with results: rows represent the real values and columns the predictions. \n",
    "* In the binary case, the cells are called *True Positive* (TP), *False Negative* (FN: Type 2 error), *False Positive* (FN: Type 1 error) and *True Negative* (TN)\n",
    "* So in the example, TP=3, FN=4, FP=3 and TN=3.\n",
    "example |  Confusion matrix\n",
    ":--:|:--:\n",
    "<img src=images/Prec-recall.png width=60%>|<img src=images/conf_mat.png width=60%>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea47d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following measurements can be derived from these:\n",
    "\n",
    "* **Accuracy**: Percentage of correct predictions -> (TP+TN)/(TP+FP+FN+TN).\n",
    "* **un- / weighted Recall/Sensitivity**: percentage of detected cases -> TP / (TP+FN). Can be weighted by class frequency, for multiple classes \n",
    "* **un- / weighted Precision**: percentage of relevant predictions -> TP / (TP+FP)\n",
    "* **Specificity**: Like sensitivity, but for the negative examples -> TN / (TN+FP)\n",
    "* **F1**: Combination of Recall and Precision -> F1 = 2 * (Rec* Prec)/ (Rec + Prec) \n",
    "* **AUC/ROC** Usually there's a tradeoff between Recall and Precision. With the *Receiver Operator Curve*  and it's *Area under curve* this can be visualized by plotting the False positive rate (100-specificity) against the True positive rate (sensitivity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b8dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression\n",
    "* **Pearson's** Pearson's Correlation Coefficient  measures the similarity of two sets of numbers with the same lenght. I's a value between -1 and 1, with 0 meaning no correlation and -1 negative correlation. When plotted in 2-d space, PCC=1 would be the identity line.\n",
    "\n",
    "* **MAE** Mean absolute error: taken two sets of numbers with same length as correct and predicted values, one can compute the mean absolute error by summing up the absolute values of the pairwise differences and scale by the number of samples.\n",
    "\n",
    "* **CCC** Concordance Correlation Coefficient is a measure quite similar to PCC but tries to penalize rater bias (seeing the two distributions as truth and ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc54d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Approaches\n",
    "### Train / test /dev splits\n",
    "Best you have enough data to split it into seperate sets:\n",
    "* **train** for the training\n",
    "* **dev** to tune meta-parameters\n",
    "* **test** as a final test set \n",
    "\n",
    "Be careful to make sure that they are  speaker disjunct, i.e. not have overlapping speakers, else you can't be sure if you learn general speaker characteristics or speaker idiosyncrasies.\n",
    "\n",
    "Also it's a very good idea to have the test set from a completely different data source, so you could have more trust in the generalizability of your model.\n",
    "\n",
    "More [on the subject here](http://blog.syntheticspeech.de/2022/12/01/how-to-split-you-data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73ef57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### X fold cross validation\n",
    "\n",
    "* If you are low on data, you might try x fold cross validation, \n",
    "* it means that you split your data  in *x* (usually 10) sets with same size, \n",
    "* and then do *x* trainings, \n",
    "* using one set as *dev set* and the rest for *train*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc46a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LOSO\n",
    "\n",
    "* *Leave one Speaker out* \n",
    "* is like X fold cross-validation, but each set are all samples of one speaker. \n",
    "* If there are many speakers, you might want *Leave one speaker group out*.\n",
    "Both is supported by [Nkululeko](http://blog.syntheticspeech.de/2021/08/04/machine-learning-experiment-framework/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba17a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
