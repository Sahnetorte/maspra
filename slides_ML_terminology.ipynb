{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169ce4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning\n",
    "## terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26e52d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function\n",
    " is the function that artificial neural nets use to track progress, i.e. the function that evaluates the predicted outcome with the desired one. Finding a good loss function is crucial for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87be2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "Fundamental way to train neural networks by evaluating the error with the loss function and than propagating it backwards towards the input layer, by taking the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f616679",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch size\n",
    "number of samples in one batch in the training which are used together to compute the error (-> loss function) and do the backpropagation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6b464",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Embeddings\n",
    " are learned representations of data, usually the pen-ultimate layer of a pretrained artificial neural net. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b215c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Latent space\n",
    " means the property of deep artificial neural nets to represent specific features of the data within the higher layers, for example speaker characteristics or expressed emotion in a net trained for speech synthesis. This is often used to influence the output in a desired way, for example simulating a specific speaking style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6cf51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Freezing\n",
    " layers in an ANN means to not update the weights, as they might contain knowledge that should not be forgotten (from a pretrained net) or to make the training faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5be0ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Drop out\n",
    "is the technique to delete a number of randomly selected neurons in a hidden layer during training to prevent [overfitting](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2133df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Patience \n",
    "* Number of epochs with no improvement after which training will be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc3eeb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overfitting\n",
    "* Means that the machine learner performs well on the training but not on any other data. \n",
    "* This is usually the case when the model has enough complexity to distinguish all training data and is trained for enough periods (one period is one run through the training). \n",
    "* Measures against this are subsumed under the label *regularization*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fbd54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vanishing / exploding gradient \n",
    "Means that the weights of the neurons become too small or too large for the net to be stable. \n",
    "\n",
    "This happens especially with very deep (many layers) networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819ac51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias vs. variance \n",
    " means the trade-off between generalization (high bias, underfitting) and specification (high variance, overfitting). You can either \n",
    " \n",
    "* have simple models, like e.g. linear regression classifiers, that will treat every input with a similar strong bias (wrong decisions), irrespective of the training set, or \n",
    "* very complex models (e.g. a neural net with many layers) that will be more exact but very specific to your training data.\n",
    "\n",
    "[Here](https://mlu-explain.github.io/bias-variance/)'s a nice visualization of bias vs. variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba17a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
